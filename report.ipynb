{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from experiments.train import train\n",
    "from experiments.evaluate import evaluate\n",
    "from agents.SARSA_agent import SARSAAgent\n",
    "from agents.Q_agent import Qlearning_agent \n",
    "from experiments.config import ENV_CONFIG, TRAINING_CONFIG, EVAL_CONFIG\n",
    "from experiments.run_baselines import run_baselines\n",
    "from environnement.cournot_env import CournotEnv\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceeb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rewards = run_baselines()\n",
    "baseline_avg_profit = baseline_rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards_sarsa, agents_sarsa = train(\n",
    "    agent_class=SARSAAgent,\n",
    "    agent_kwargs={\"n_actions\": 1, \"q_max\": ENV_CONFIG[\"q_max\"]}\n",
    ")\n",
    "\n",
    "eval_rewards_sarsa = evaluate(agents_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f639c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards_q, agents_q = train(\n",
    "    agent_class=Qlearning_agent,\n",
    "    agent_kwargs={\"q_max\": ENV_CONFIG[\"q_max\"]}\n",
    ")\n",
    "\n",
    "eval_rewards_q = evaluate(agents_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sarsa = np.mean(episode_rewards_sarsa, axis=1)\n",
    "avg_q = np.mean(episode_rewards_q, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(avg_sarsa, label=\"SARSA\")\n",
    "plt.plot(avg_q, label=\"Q-learning\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average profit per firm\")\n",
    "plt.title(\"Learning curves comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89752be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_episode(env, agents, greedy=True):\n",
    "    \"\"\"\n",
    "    Rollout one episode and record states, actions, rewards, info.\n",
    "    If greedy=True, sets epsilon=0 temporarily (if exists).\n",
    "    \"\"\"\n",
    "    # turn off exploration\n",
    "    old_eps = []\n",
    "    if greedy:\n",
    "        for ag in agents:\n",
    "            if hasattr(ag, \"epsilon\"):\n",
    "                old_eps.append(ag.epsilon)\n",
    "                ag.epsilon = 0.0\n",
    "            else:\n",
    "                old_eps.append(None)\n",
    "\n",
    "    states = []\n",
    "    actions_hist = []\n",
    "    rewards_hist = []\n",
    "    prices = []\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        states.append(state.copy())\n",
    "\n",
    "        actions = np.array([ag.select_action(state)[0] for ag in agents])\n",
    "        actions_hist.append(actions.copy())\n",
    "\n",
    "        next_state, rewards, done, info = env.step(actions)\n",
    "        rewards_hist.append(rewards.copy())\n",
    "        prices.append(info[\"price\"])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # restore epsilons\n",
    "    if greedy:\n",
    "        for ag, eps in zip(agents, old_eps):\n",
    "            if eps is not None:\n",
    "                ag.epsilon = eps\n",
    "\n",
    "    return (np.array(states), np.array(actions_hist), np.array(rewards_hist), np.array(prices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e71266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.metrics import nash_quantities, nash_price, distance_to_nash\n",
    "from analysis.plotting import (\n",
    "    plot_quantities_vs_nash,\n",
    "    plot_price_vs_nash,\n",
    "    plot_distance_to_nash,\n",
    "    plot_distance_to_nash_rolling\n",
    ")\n",
    "\n",
    "env = CournotEnv(**ENV_CONFIG)\n",
    "\n",
    "# Choose which trained agents you want to analyze:\n",
    "agents = agents_sarsa   # or agents_q\n",
    "\n",
    "states, actions_hist, rewards_hist, prices = rollout_episode(env, agents, greedy=True)\n",
    "\n",
    "q_nash = nash_quantities(ENV_CONFIG)\n",
    "p_nash = nash_price(ENV_CONFIG)\n",
    "\n",
    "# Distance to Nash based on actions (quantities actually chosen at time t)\n",
    "dist = distance_to_nash(actions_hist, q_nash, metric=\"l2\")\n",
    "\n",
    "print(\"Nash quantities:\", q_nash)\n",
    "print(\"Nash price:\", p_nash)\n",
    "print(\"Mean L2 distance to Nash:\", dist.mean())\n",
    "\n",
    "plot_quantities_vs_nash(actions_hist, q_nash, title=\"Quantities (chosen) vs Nash\")\n",
    "plot_price_vs_nash(prices, p_nash, title=\"Price vs Nash\")\n",
    "plot_distance_to_nash(dist, title=\"Distance to Nash (per step)\")\n",
    "plot_distance_to_nash_rolling(dist, window=10, title=\"Distance to Nash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98841f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_distance_distribution(agents, n_eval_episodes=50):\n",
    "    env = CournotEnv(**ENV_CONFIG)\n",
    "    q_nash = nash_quantities(ENV_CONFIG)\n",
    "\n",
    "    means = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        _, actions_hist, _, _ = rollout_episode(env, agents, greedy=True)\n",
    "        dist = distance_to_nash(actions_hist, q_nash, metric=\"l2\")\n",
    "        means.append(dist.mean())\n",
    "\n",
    "    return np.array(means)\n",
    "\n",
    "dists = eval_distance_distribution(agents, n_eval_episodes=50)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(dists, bins=15)\n",
    "plt.xlabel(\"Mean L2 distance to Nash (episode)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Nash distances across evaluation episodes\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Average mean distance:\", dists.mean())\n",
    "print(\"Std:\", dists.std())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
